{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install & Import Dependencies"
      ],
      "metadata": {
        "id": "KRC9Nzec8Vbl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RuN4HRf1qMqe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "088c696d-1daa-4c27-d2f7-09f61ae124f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/487.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m481.3/487.4 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers datasets accelerate evaluate torch\n",
        "!pip install -q transformers datasets accelerate evaluate torch contractions\n",
        "!pip install -q nlpaug"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import unicodedata\n",
        "import contractions\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "import nlpaug.augmenter.word as naw  # For data augmentation\n",
        "from datasets import Dataset as HFDataset\n",
        "\n",
        "# Add NLTK resource download\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('wordnet')\n",
        "print(\"NLTK resources downloaded successfully\")\n"
      ],
      "metadata": {
        "id": "OSGxxVae97Lz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c8996a4-5a36-401d-8883-098fe56c832f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK resources downloaded successfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conecting to drive"
      ],
      "metadata": {
        "id": "bfg5Eg_T8XwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgAaLc0kODmv",
        "outputId": "a131062b-db53-4459-f6e5-d3f85eb45cb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Data & inspecting data"
      ],
      "metadata": {
        "id": "Ia7R4clF-TrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = \"/content/training_set_rel3.tsv\"\n",
        "valid_path = \"/content/valid_set.tsv\"\n",
        "test_path = \"/content/test_set.tsv\"\n",
        "\n",
        "# Loading TSV files with tab separator and ISO-8859-1 encoding for compatibility.\n",
        "train_df = pd.read_csv(train_path, sep=\"\\t\", encoding=\"ISO-8859-1\") # Tab-separated handles Western chars.\n",
        "valid_df = pd.read_csv(valid_path, sep=\"\\t\", encoding=\"ISO-8859-1\")\n",
        "test_df = pd.read_csv(test_path, sep=\"\\t\", encoding=\"ISO-8859-1\")\n",
        "print(\"Datasets loaded successfully \")"
      ],
      "metadata": {
        "id": "461GaHrsrNfN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3430d03c-0089-4582-a6ba-b3cd8d4e9e37"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets loaded successfully \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train Data Columns:\", train_df.columns.tolist())\n",
        "print(\"Valid Data Columns:\", valid_df.columns.tolist())\n",
        "print(\"Test Data Columns:\", test_df.columns.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnCNyj5LrvO0",
        "outputId": "acec00f5-767d-4a61-b3b2-39407d257c7e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data Columns: ['essay_id', 'essay_set', 'essay', 'rater1_domain1', 'rater2_domain1', 'rater3_domain1', 'domain1_score', 'rater1_domain2', 'rater2_domain2', 'domain2_score', 'rater1_trait1', 'rater1_trait2', 'rater1_trait3', 'rater1_trait4', 'rater1_trait5', 'rater1_trait6', 'rater2_trait1', 'rater2_trait2', 'rater2_trait3', 'rater2_trait4', 'rater2_trait5', 'rater2_trait6', 'rater3_trait1', 'rater3_trait2', 'rater3_trait3', 'rater3_trait4', 'rater3_trait5', 'rater3_trait6']\n",
            "Valid Data Columns: ['essay_id', 'essay_set', 'essay', 'domain1_predictionid', 'domain2_predictionid']\n",
            "Test Data Columns: ['essay_id', 'essay_set', 'essay', 'domain1_predictionid', 'domain2_predictionid']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(valid_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c5J10SMsLIJ",
        "outputId": "062bc8e2-2483-40d1-bcbc-f46e220fb8c1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   essay_id  essay_set                                              essay  \\\n",
            "0      1788          1  Dear @ORGANIZATION1, @CAPS1 more and more peop...   \n",
            "1      1789          1  Dear @LOCATION1 Time @CAPS1 me tell you what I...   \n",
            "2      1790          1  Dear Local newspaper, Have you been spending a...   \n",
            "3      1791          1  Dear Readers, @CAPS1 you imagine how life woul...   \n",
            "4      1792          1  Dear newspaper, I strongly believe that comput...   \n",
            "\n",
            "   domain1_predictionid  domain2_predictionid  \n",
            "0                  1788                   NaN  \n",
            "1                  1789                   NaN  \n",
            "2                  1790                   NaN  \n",
            "3                  1791                   NaN  \n",
            "4                  1792                   NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select Relevant Features"
      ],
      "metadata": {
        "id": "bb6E0DW5_778"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df[['essay', 'domain1_score', 'essay_set']]"
      ],
      "metadata": {
        "id": "2xWGJTxjwVtg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.dropna(subset=['essay', 'domain1_score', 'essay_set'], inplace=True)\n",
        "train_df.drop_duplicates(subset=['essay'], inplace=True)\n",
        "train_df['domain1_score'] = train_df['domain1_score'].astype(float)"
      ],
      "metadata": {
        "id": "W4omlCZpwXax"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['normalized_score'] = train_df.groupby('essay_set')['domain1_score'].transform(\n",
        "    lambda x: scaler.fit_transform(x.values.reshape(-1, 1)).flatten()\n",
        ")"
      ],
      "metadata": {
        "id": "zG4jpZrawZPr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definingg data augmentation function\n",
        "def augment_text(text, aug_p=0.3):\n",
        "    aug = naw.SynonymAug(aug_p=aug_p)  # Synonym replacement\n",
        "    augmented_text = aug.augment(text)\n",
        "    return augmented_text[0] if isinstance(augmented_text, list) else augmented_text\n",
        "\n",
        "# Applying augmentation to training data\n",
        "def augment_dataset(df, num_augmentations=1):\n",
        "    augmented_data = []\n",
        "    for _, row in df.iterrows():\n",
        "        essay = row['essay']\n",
        "        domain1_score = row['domain1_score']\n",
        "        essay_set = row['essay_set']\n",
        "        normalized_score = row['normalized_score']  # just ensuring normalized_score is included\n",
        "        # Original sample\n",
        "        augmented_data.append({\n",
        "            'essay': essay,\n",
        "            'domain1_score': domain1_score,\n",
        "            'essay_set': essay_set,\n",
        "            'normalized_score': normalized_score\n",
        "        })\n",
        "        # Augmented samples\n",
        "        for _ in range(num_augmentations):\n",
        "            augmented_essay = augment_text(essay)\n",
        "            augmented_data.append({\n",
        "                'essay': augmented_essay,\n",
        "                'domain1_score': domain1_score,\n",
        "                'essay_set': essay_set,\n",
        "                'normalized_score': normalized_score\n",
        "            })\n",
        "    return pd.DataFrame(augmented_data)\n",
        "\n",
        "# applying augmentation to train_df\n",
        "print(\"Augmenting dataset...\")\n",
        "train_df = augment_dataset(train_df, num_augmentations=1)  # Double the dataset size\n",
        "print(\"Augmented Train Data Shape:\", train_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3aOMRRHv2Th",
        "outputId": "745d159e-1a7c-4288-e0c4-c434a687dc69"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmenting dataset...\n",
            "Augmented Train Data Shape: (25944, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('wordnet')  #its required for SynonymAug\n",
        "print(\"NLTK resources downloaded successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLHnpx6MxqYJ",
        "outputId": "94b2c4e1-7a70-44fb-9e4d-e5e62bc4e39c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK resources downloaded successfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Selectingg Relevant Features\n",
        "train_df = train_df[['essay', 'domain1_score', 'essay_set']].copy()  # Creating a copy to avoid SettingWithCopyWarning\n",
        "\n",
        "# Dropping rows with missing values\n",
        "train_df = train_df.dropna(subset=['essay', 'domain1_score', 'essay_set'])\n",
        "\n",
        "# Dropping duplicate essays\n",
        "train_df = train_df.drop_duplicates(subset=['essay'])\n",
        "\n",
        "# Ensuring target variable is in float format\n",
        "train_df.loc[:, 'domain1_score'] = train_df['domain1_score'].astype(float)\n",
        "\n",
        "# Normalizing scores per essay_set (0–1 range)\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "train_df.loc[:, 'normalized_score'] = train_df.groupby('essay_set')['domain1_score'].transform(\n",
        "    lambda x: scaler.fit_transform(x.values.reshape(-1, 1)).flatten()\n",
        ")"
      ],
      "metadata": {
        "id": "yLDzpR_hv-C2"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = contractions.fix(text)  # Expanding contractions\n",
        "    text = unicodedata.normalize(\"NFKD\", text)\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Removing punctuation and special characters\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Removing extra whitespace\n",
        "    return text\n",
        "\n",
        "\n",
        "train_df = train_df[['essay', 'domain1_score', 'essay_set']].copy()\n",
        "\n",
        "train_df = train_df.dropna(subset=['essay', 'domain1_score', 'essay_set'])\n",
        "\n",
        "train_df = train_df.drop_duplicates(subset=['essay'])\n",
        "\n",
        "# Ensuriing target variable is in float format\n",
        "train_df.loc[:, 'domain1_score'] = train_df['domain1_score'].astype(float)\n",
        "\n",
        "# Normalizing scores per essay_set (0–1 range)\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "train_df.loc[:, 'normalized_score'] = train_df.groupby('essay_set')['domain1_score'].transform(\n",
        "    lambda x: scaler.fit_transform(x.values.reshape(-1, 1)).flatten()\n",
        ")\n",
        "\n",
        "# Apply text cleaning (after augmentation to ensure consistency)\n",
        "train_df.loc[:, 'essay'] = train_df['essay'].apply(clean_text)\n",
        "print(\"Text Cleaning: Normalization & Formatting Done\")\n",
        "\n",
        "# Prepare for k-fold cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "folds = list(kf.split(train_df))\n",
        "print(\"K-Fold Cross-Validation Prepared with 5 Folds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkxbVeTayujm",
        "outputId": "01bdfd14-9293-400a-a4f9-d2472cd3d40b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Cleaning: Normalization & Formatting Done\n",
            "K-Fold Cross-Validation Prepared with 5 Folds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Cleaning Function"
      ],
      "metadata": {
        "id": "-6Ws1VHA_5bi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = contractions.fix(text)  # Expanding contractions.\n",
        "    text = unicodedata.normalize(\"NFKD\", text)\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Removing punctuation and special characters.\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Removing extra whitespace.\n",
        "    return text\n",
        "\n",
        "train_data['essay'] = train_data['essay'].apply(clean_text)\n",
        "valid_data['essay'] = valid_data['essay'].apply(clean_text)\n",
        "test_df['essay'] = test_df['essay'].apply(clean_text)\n",
        "print(\"Text Cleaning: Normalization & Formatting Done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvSc-oFUsr2B",
        "outputId": "e5c9568a-5520-47a6-ca5a-c33aed5095fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Cleaning: Normalization & Formatting Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization & Truncation"
      ],
      "metadata": {
        "id": "mOmJx1Pf_3iU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply augmentation to train_df\n",
        "print(\"Augmenting dataset...\")\n",
        "train_df = augment_dataset(train_df, num_augmentations=0)  # Skipping augmentation\n",
        "print(\"Train Data Shape (No Augmentation):\", train_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxqkj9r09TyN",
        "outputId": "7f871bd5-ee2b-4a90-afd4-2c308e7365e9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmenting dataset...\n",
            "Train Data Shape (No Augmentation): (25944, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**I have forgot to save the results of this cell and overwrite it by the cell after, however I had screenshot for its results and mentiond it in the report**"
      ],
      "metadata": {
        "id": "BWeH0Ae3IGTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer\n",
        "model_name = \"microsoft/deberta-v3-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/Colab Notebooks/deberta-aes\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=3,\n",
        "    learning_rate=3e-5,  # Adjusted learning rate\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,  # Increase epochs for better convergence\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"none\",\n",
        "    fp16=True,\n",
        "    logging_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "# Define metrics for evaluation\n",
        "import numpy as np\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.squeeze(predictions)\n",
        "    labels = labels.astype(float)\n",
        "    mse = np.mean((predictions - labels) ** 2)\n",
        "    mae = np.mean(np.abs(predictions - labels))\n",
        "    predictions_rounded = np.round(predictions * 10).astype(int)  # Scale 0–1 to 0–10\n",
        "    labels_rounded = np.round(labels * 10).astype(int)\n",
        "    qwk_score = cohen_kappa_score(labels_rounded, predictions_rounded, weights=\"quadratic\")\n",
        "    return {\n",
        "        \"mse\": mse,\n",
        "        \"mae\": mae,\n",
        "        \"qwk\": qwk_score\n",
        "    }\n",
        "\n",
        "# K-Fold Cross-Validation Training Loop\n",
        "fold_results = []\n",
        "for fold, (train_idx, val_idx) in enumerate(folds):\n",
        "    print(f\"\\nTraining Fold {fold + 1}/{len(folds)}\")\n",
        "\n",
        "    # Split data for this fold\n",
        "    train_data = train_df.iloc[train_idx]\n",
        "    valid_data = train_df.iloc[val_idx]\n",
        "\n",
        "    # Create datasets using normalized_score\n",
        "    train_dataset = EssayDataset(train_data[\"essay\"].tolist(), train_data[\"normalized_score\"].tolist())\n",
        "    valid_dataset = EssayDataset(valid_data[\"essay\"].tolist(), valid_data[\"normalized_score\"].tolist())\n",
        "\n",
        "    # Convert to Hugging Face Dataset format\n",
        "    train_hf_dataset = HFDataset.from_dict({\n",
        "        \"input_ids\": train_dataset.encodings[\"input_ids\"].tolist(),\n",
        "        \"attention_mask\": train_dataset.encodings[\"attention_mask\"].tolist(),\n",
        "        \"labels\": train_dataset.labels.tolist()\n",
        "    })\n",
        "    valid_hf_dataset = HFDataset.from_dict({\n",
        "        \"input_ids\": valid_dataset.encodings[\"input_ids\"].tolist(),\n",
        "        \"attention_mask\": valid_dataset.encodings[\"attention_mask\"].tolist(),\n",
        "        \"labels\": valid_dataset.labels.tolist()\n",
        "    })\n",
        "\n",
        "    # Reset model for each fold\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_hf_dataset,\n",
        "        eval_dataset=valid_hf_dataset,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(f\"Fold {fold + 1} Evaluation Results:\", eval_results)\n",
        "    fold_results.append(eval_results)\n",
        "\n",
        "    # Check sample predictions (fix the slicing issue)\n",
        "    valid_subset = valid_hf_dataset.select(range(min(10, len(valid_hf_dataset))))\n",
        "    predictions = trainer.predict(valid_subset).predictions\n",
        "    print(f\"Fold {fold + 1} Sample Predictions:\", predictions.flatten())\n",
        "\n",
        "# Average results across folds\n",
        "avg_results = {\n",
        "    \"avg_mse\": np.mean([r[\"eval_mse\"] for r in fold_results]),\n",
        "    \"avg_mae\": np.mean([r[\"eval_mae\"] for r in fold_results]),\n",
        "    \"avg_qwk\": np.mean([r[\"eval_qwk\"] for r in fold_results])\n",
        "}\n",
        "print(\"\\nAverage Results Across Folds:\", avg_results)"
      ],
      "metadata": {
        "id": "tuxIQxPQH4-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**this one gives close results, after adjusting the Lr and minor things like the weight_decay for handling overfitting and so on**"
      ],
      "metadata": {
        "id": "d19pXxkBJIh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**tried to train only one fold for both cells due to the significant power (2hrs for the 3epchos on A100)**"
      ],
      "metadata": {
        "id": "bEAo1PFkMB1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer\n",
        "model_name = \"microsoft/deberta-v3-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/Colab Notebooks/deberta-aes\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=3,\n",
        "    learning_rate=3e-5,  # Adjusted learning rate\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,  # Increase epochs for better convergence\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"none\",\n",
        "    fp16=True,\n",
        "    logging_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "# Define metrics for evaluation\n",
        "import numpy as np\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.squeeze(predictions)\n",
        "    labels = labels.astype(float)\n",
        "    mse = np.mean((predictions - labels) ** 2)\n",
        "    mae = np.mean(np.abs(predictions - labels))\n",
        "    predictions_rounded = np.round(predictions * 10).astype(int)  # Scale 0–1 to 0–10\n",
        "    labels_rounded = np.round(labels * 10).astype(int)\n",
        "    qwk_score = cohen_kappa_score(labels_rounded, predictions_rounded, weights=\"quadratic\")\n",
        "    return {\n",
        "        \"mse\": mse,\n",
        "        \"mae\": mae,\n",
        "        \"qwk\": qwk_score\n",
        "    }\n",
        "\n",
        "# K-Fold Cross-Validation Training Loop\n",
        "fold_results = []\n",
        "for fold, (train_idx, val_idx) in enumerate(folds):\n",
        "    print(f\"\\nTraining Fold {fold + 1}/{len(folds)}\")\n",
        "\n",
        "    # Split data for this fold\n",
        "    train_data = train_df.iloc[train_idx]\n",
        "    valid_data = train_df.iloc[val_idx]\n",
        "\n",
        "    # Create datasets using normalized_score\n",
        "    train_dataset = EssayDataset(train_data[\"essay\"].tolist(), train_data[\"normalized_score\"].tolist())\n",
        "    valid_dataset = EssayDataset(valid_data[\"essay\"].tolist(), valid_data[\"normalized_score\"].tolist())\n",
        "\n",
        "    # Convert to Hugging Face Dataset format\n",
        "    train_hf_dataset = HFDataset.from_dict({\n",
        "        \"input_ids\": train_dataset.encodings[\"input_ids\"].tolist(),\n",
        "        \"attention_mask\": train_dataset.encodings[\"attention_mask\"].tolist(),\n",
        "        \"labels\": train_dataset.labels.tolist()\n",
        "    })\n",
        "    valid_hf_dataset = HFDataset.from_dict({\n",
        "        \"input_ids\": valid_dataset.encodings[\"input_ids\"].tolist(),\n",
        "        \"attention_mask\": valid_dataset.encodings[\"attention_mask\"].tolist(),\n",
        "        \"labels\": valid_dataset.labels.tolist()\n",
        "    })\n",
        "\n",
        "    # Reset model for each fold\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_hf_dataset,\n",
        "        eval_dataset=valid_hf_dataset,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(f\"Fold {fold + 1} Evaluation Results:\", eval_results)\n",
        "    fold_results.append(eval_results)\n",
        "\n",
        "    # Check sample predictions (fix the slicing issue)\n",
        "    valid_subset = valid_hf_dataset.select(range(min(10, len(valid_hf_dataset))))\n",
        "    predictions = trainer.predict(valid_subset).predictions\n",
        "    print(f\"Fold {fold + 1} Sample Predictions:\", predictions.flatten())\n",
        "\n",
        "# Average results across folds\n",
        "avg_results = {\n",
        "    \"avg_mse\": np.mean([r[\"eval_mse\"] for r in fold_results]),\n",
        "    \"avg_mae\": np.mean([r[\"eval_mae\"] for r in fold_results]),\n",
        "    \"avg_qwk\": np.mean([r[\"eval_qwk\"] for r in fold_results])\n",
        "}\n",
        "print(\"\\nAverage Results Across Folds:\", avg_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MyTwmLeWy9Ej",
        "outputId": "34a653b6-ed9c-47be-84a6-13313deafe68"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Fold 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15567' max='15567' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [15567/15567 55:01, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Mse</th>\n",
              "      <th>Mae</th>\n",
              "      <th>Qwk</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.055100</td>\n",
              "      <td>0.065596</td>\n",
              "      <td>0.065596</td>\n",
              "      <td>0.200046</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.041500</td>\n",
              "      <td>0.058801</td>\n",
              "      <td>0.058801</td>\n",
              "      <td>0.190596</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.038900</td>\n",
              "      <td>0.057381</td>\n",
              "      <td>0.057381</td>\n",
              "      <td>0.188589</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Predictions (first 10): [0.68310547 0.68310547 0.68310547 0.68310547 0.68310547 0.68310547\n",
            " 0.68310547 0.68310547 0.68310547 0.68310547]\n",
            "Sample Labels (first 10): [0.69999999 0.40000001 1.         0.60000002 0.60000002 0.80000001\n",
            " 0.89999998 0.2        0.69999999 0.69999999]\n",
            "Rounded Predictions (first 10): [7 7 7 7 7 7 7 7 7 7]\n",
            "Rounded Labels (first 10): [ 7  4 10  6  6  8  9  2  7  7]\n",
            "Sample Predictions (first 10): [0.6328125 0.6328125 0.6328125 0.6328125 0.6328125 0.6328125 0.6328125\n",
            " 0.6328125 0.6328125 0.6328125]\n",
            "Sample Labels (first 10): [0.69999999 0.40000001 1.         0.60000002 0.60000002 0.80000001\n",
            " 0.89999998 0.2        0.69999999 0.69999999]\n",
            "Rounded Predictions (first 10): [6 6 6 6 6 6 6 6 6 6]\n",
            "Rounded Labels (first 10): [ 7  4 10  6  6  8  9  2  7  7]\n",
            "Sample Predictions (first 10): [0.6098633 0.6098633 0.6098633 0.6098633 0.6098633 0.6098633 0.6098633\n",
            " 0.6098633 0.6098633 0.6098633]\n",
            "Sample Labels (first 10): [0.69999999 0.40000001 1.         0.60000002 0.60000002 0.80000001\n",
            " 0.89999998 0.2        0.69999999 0.69999999]\n",
            "Rounded Predictions (first 10): [6 6 6 6 6 6 6 6 6 6]\n",
            "Rounded Labels (first 10): [ 7  4 10  6  6  8  9  2  7  7]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Predictions (first 10): [0.6098633 0.6098633 0.6098633 0.6098633 0.6098633 0.6098633 0.6098633\n",
            " 0.6098633 0.6098633 0.6098633]\n",
            "Sample Labels (first 10): [0.69999999 0.40000001 1.         0.60000002 0.60000002 0.80000001\n",
            " 0.89999998 0.2        0.69999999 0.69999999]\n",
            "Rounded Predictions (first 10): [6 6 6 6 6 6 6 6 6 6]\n",
            "Rounded Labels (first 10): [ 7  4 10  6  6  8  9  2  7  7]\n",
            "Fold 1 Evaluation Results: {'eval_loss': 0.05738134682178497, 'eval_mse': 0.057381345178923286, 'eval_mae': 0.18858916465229647, 'eval_qwk': 0.0, 'eval_runtime': 82.7571, 'eval_samples_per_second': 62.702, 'eval_steps_per_second': 15.684, 'epoch': 3.0}\n",
            "Sample Predictions (first 10): [0.6098633 0.6098633 0.6098633 0.6098633 0.6098633 0.6098633 0.6098633\n",
            " 0.6098633 0.6098633 0.6098633]\n",
            "Sample Labels (first 10): [0.69999999 0.40000001 1.         0.60000002 0.60000002 0.80000001\n",
            " 0.89999998 0.2        0.69999999 0.69999999]\n",
            "Rounded Predictions (first 10): [6 6 6 6 6 6 6 6 6 6]\n",
            "Rounded Labels (first 10): [ 7  4 10  6  6  8  9  2  7  7]\n",
            "Fold 1 Sample Predictions: [0.6098633 0.6098633 0.6098633 0.6098633 0.6098633 0.6098633 0.6098633\n",
            " 0.6098633 0.6098633 0.6098633]\n",
            "\n",
            "Training Fold 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5263' max='15567' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 5263/15567 18:19 < 35:54, 4.78 it/s, Epoch 1.01/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Mse</th>\n",
              "      <th>Mae</th>\n",
              "      <th>Qwk</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.054600</td>\n",
              "      <td>0.105663</td>\n",
              "      <td>0.105663</td>\n",
              "      <td>0.266249</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Predictions (first 10): [0.8120117 0.8120117 0.8120117 0.8120117 0.8120117 0.8120117 0.8120117\n",
            " 0.8120117 0.8120117 0.8120117]\n",
            "Sample Labels (first 10): [0.60000002 0.69999999 0.80000001 0.69999999 1.         0.40000001\n",
            " 0.60000002 0.1        0.89999998 0.60000002]\n",
            "Rounded Predictions (first 10): [8 8 8 8 8 8 8 8 8 8]\n",
            "Rounded Labels (first 10): [ 6  7  8  7 10  4  6  1  9  6]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-29bd57df6074>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;31m# Evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2240\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2241\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2242\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2546\u001b[0m                     )\n\u001b[1;32m   2547\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2548\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3738\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3740\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3742\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2353\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2355\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2356\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_lomo_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2357\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Model"
      ],
      "metadata": {
        "id": "IAp685NTDFoj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Post-Training Evaluation"
      ],
      "metadata": {
        "id": "IoHMB9w-kkOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert EssayDataset to a format Trainer expects\n",
        "from datasets import Dataset as HFDataset\n",
        "\n",
        "train_hf_dataset = HFDataset.from_dict({\n",
        "    \"input_ids\": train_dataset.encodings[\"input_ids\"].tolist(),\n",
        "    \"attention_mask\": train_dataset.encodings[\"attention_mask\"].tolist(),\n",
        "    \"labels\": train_dataset.labels.tolist()\n",
        "})\n",
        "valid_hf_dataset = HFDataset.from_dict({\n",
        "    \"input_ids\": valid_dataset.encodings[\"input_ids\"].tolist(),\n",
        "    \"attention_mask\": valid_dataset.encodings[\"attention_mask\"].tolist(),\n",
        "    \"labels\": valid_dataset.labels.tolist()\n",
        "})\n",
        "\n",
        "# Single run\n",
        "batch_size = 4\n",
        "learning_rate = 3e-5\n",
        "\n",
        "print(f\"Training with Batch Size: {batch_size}, Learning Rate: {learning_rate}\")\n",
        "training_args.per_device_train_batch_size = batch_size\n",
        "training_args.per_device_eval_batch_size = batch_size\n",
        "training_args.learning_rate = learning_rate\n",
        "\n",
        "# Reset model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
        "trainer.model = model\n",
        "\n",
        "# Update trainer datasets\n",
        "trainer.train_dataset = train_hf_dataset\n",
        "trainer.eval_dataset = valid_hf_dataset\n",
        "\n",
        "# Train\n",
        "trainer.train()\n",
        "\n",
        "# Check predictions to debug constant eval loss\n",
        "predictions = trainer.predict(valid_hf_dataset[:10]).predictions\n",
        "print(\"Sample Predictions after 1 epoch:\", predictions.flatten())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "id": "xNkYhXbx2P39",
        "outputId": "54ebbd3a-5fa2-43f9-af18-7adee7c2ddb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with Batch Size: 4, Learning Rate: 3e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2270/2270 16:29, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Model Preparation Time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.258700</td>\n",
              "      <td>0.079961</td>\n",
              "      <td>0.441300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-888ad4eb7175>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Check predictions to debug constant eval loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_hf_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample Predictions after 1 epoch:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4182\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4183\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   4184\u001b[0m             \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Prediction\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4185\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4288\u001b[0m         \u001b[0;31m# Main evaluation loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4289\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4290\u001b[0m             \u001b[0;31m# Update the observed num examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4291\u001b[0m             \u001b[0mobserved_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;31m# We iterate one batch ahead to check when we are at the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             \u001b[0mcurrent_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 0"
          ]
        }
      ]
    }
  ]
}